{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The demonstrative notebook for Hive assignments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run any HiveQL query in the notebook you should:\n",
    "1. write the code of query into a separate file using `%%writefile [-a] <file>` magic,\n",
    "2. execute this file in hive using `! hive -f <file>` command.\n",
    "\n",
    "To make grading system check a task correctly, execution command must be in a separate cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Creation the database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, create your Hive database. You can name the database whatever you want."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's drop database if it has already created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%writefile creation_db.hql\n",
    "\n",
    "#DROP DATABASE IF EXISTS demodb CASCADE;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now create it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%writefile -a creation_db.hql\n",
    "#CREATE DATABASE demodb LOCATION '/user/jovyan/somemetastore';"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, execute the file we filled earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! hive -f creation_db.hql"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the real Hadoop-cluster where your submission will be checked we already have precreated Hive databases for all users. This helps to avoid database name conflicts. If you're the new user, the database will be created during your first submission of Hive assignment. The system won't allow you to create your own database on Hadoop-cluster so when you submit the final version of the task you shoud **remove or comment** all the lines related to database's dropping and creation. \n",
    "\n",
    "You can left all the lines with `USE` without any changes. The grading system will replace database's name to name of the precreated database. In assignments 2 and 3 you'll need to use `stackoverflow_` database. This database's name will not be changed by the grading system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Creation the external table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us our source dataset have 2 collumns:\n",
    "* ip-address,\n",
    "* its subnet's mask.\n",
    "\n",
    "For example:\n",
    "```\n",
    "148.45.113.216\t255.255.255.248\n",
    "203.98.141.0\t255.255.255.240\n",
    "183.168.36.0\t255.255.255.128\n",
    "111.157.172.232\t255.255.255.248\n",
    "80.46.87.0\t255.255.255.0\n",
    "247.248.233.0\t255.255.255.128\n",
    "```\n",
    "Now we'll create the external table with 2 fields: ip and mask."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%%writefile exteral_table.hql\n",
    "\n",
    "ADD JAR /opt/cloudera/parcels/CDH/lib/hive/lib/hive-contrib.jar;\n",
    "\n",
    "USE demodb;\n",
    "DROP TABLE IF EXISTS Subnets;\n",
    "\n",
    "CREATE EXTERNAL TABLE Subnets (\n",
    "    ip STRING,\n",
    "    mask STRING\n",
    ")\n",
    "ROW FORMAT DELIMITED FIELDS TERMINATED BY  '\\t'\n",
    "STORED AS TEXTFILE\n",
    "LOCATION '/data/subnets/ips';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! hive -f exteral_table.hql"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Demo query on created table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's write a simpe query:\n",
    " > Compute avarage value of IPs for each subnet's mask."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%%writefile query.hql\n",
    "\n",
    "ADD JAR /opt/cloudera/parcels/CDH/lib/hive/lib/hive-contrib.jar;\n",
    "USE demodb;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%%writefile -a query.hql\n",
    "\n",
    "SELECT AVG(counts.cnt)\n",
    "FROM (\n",
    "    SELECT mask, count(ip) as cnt\n",
    "    FROM Subnets\n",
    "    GROUP BY mask\n",
    ") counts;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "! hive -f query.hql"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please take into account that the grading system catch all output (both result and MapReduce logs) from the last cell of the notebook, so **don't** redirect any output from this cell to `/dev/null`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting task1a.hql\n"
     ]
    }
   ],
   "source": [
    "%%writefile task1a.hql\n",
    "\n",
    "ADD JAR /opt/cloudera/parcels/CDH/lib/hive/lib/hive-contrib.jar;\n",
    "USE demodb;\n",
    "\n",
    "DROP TABLE IF EXISTS posts_sample_external;\n",
    "\n",
    "CREATE EXTERNAL TABLE posts_sample_external (\n",
    "    creation_year INT,\n",
    "    creation_month INT\n",
    ")\n",
    "ROW FORMAT\n",
    "    SERDE 'org.apache.hadoop.hive.serde2.RegexSerDe'\n",
    "    WITH SERDEPROPERTIES (\n",
    "        \"input.regex\" = '.*?(?=.*\\\\bCreationDate=\\\\\"(\\\\d+)-(\\\\d+)-\\\\d+T.*\\\\\").*$'\n",
    "    )\n",
    "    \n",
    "LOCATION '/data/stackexchange1000/posts'\n",
    "TBLPROPERTIES (\"skip.header.line.count\"=\"1\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting task1b.hql\n"
     ]
    }
   ],
   "source": [
    "%%writefile task1b.hql\n",
    "\n",
    "ADD JAR /opt/cloudera/parcels/CDH/lib/hive/lib/hive-contrib.jar;\n",
    "USE demodb;\n",
    "\n",
    "SET hive.exec.dynamic.partition.mode=nonstrict;\n",
    "DROP TABLE IF EXISTS posts_sample;\n",
    "\n",
    "\n",
    "CREATE TABLE posts_sample (year int, month int)\n",
    "PARTITIONED BY (creation_year INT, creation_month INT);\n",
    "\n",
    "INSERT OVERWRITE TABLE posts_sample PARTITION(creation_year, creation_month)\n",
    "SELECT\n",
    "    creation_year as year,\n",
    "    creation_month as month,\n",
    "    creation_year,\n",
    "    creation_month\n",
    "FROM posts_sample_external;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting task1c.hql\n"
     ]
    }
   ],
   "source": [
    "%%writefile task1c.hql\n",
    "\n",
    "ADD JAR /opt/cloudera/parcels/CDH/lib/hive/lib/hive-contrib.jar;\n",
    "USE demodb;\n",
    "\n",
    "SELECT t1.year, CONCAT(t1.year,'-',t1.month) as year_month, t1.lines\n",
    "FROM(\n",
    "    SELECT \n",
    "        year,\n",
    "        month,\n",
    "        count(1) as lines,\n",
    "        row_number() over(ORDER BY  month) as rn\n",
    "    FROM posts_sample\n",
    "    where creation_year = 2008\n",
    "    GROUP BY year, month\n",
    "    order by month ASC\n",
    "    limit 3)t1\n",
    "WHERE t1.rn=3;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Logging initialized using configuration in jar:file:/usr/local/apache-hive-1.1.0-bin/lib/hive-common-1.1.0.jar!/hive-log4j.properties\n",
      "Added [/opt/cloudera/parcels/CDH/lib/hive/lib/hive-contrib.jar] to class path\n",
      "Added resources: [/opt/cloudera/parcels/CDH/lib/hive/lib/hive-contrib.jar]\n",
      "OK\n",
      "Time taken: 0.56 seconds\n",
      "Query ID = jovyan_20180616232424_f44fa65a-809d-4414-946d-0e5a0c72af61\n",
      "Total jobs = 3\n",
      "Launching Job 1 out of 3\n",
      "Number of reduce tasks not specified. Estimated from input data size: 1\n",
      "In order to change the average load for a reducer (in bytes):\n",
      "  set hive.exec.reducers.bytes.per.reducer=<number>\n",
      "In order to limit the maximum number of reducers:\n",
      "  set hive.exec.reducers.max=<number>\n",
      "In order to set a constant number of reducers:\n",
      "  set mapreduce.job.reduces=<number>\n",
      "Starting Job = job_1529173785517_0056, Tracking URL = http://c8bd72c66014:8088/proxy/application_1529173785517_0056/\n",
      "Kill Command = /opt/hadoop/bin/hadoop job  -kill job_1529173785517_0056\n",
      "Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1\n",
      "2018-06-16 23:24:56,405 Stage-1 map = 0%,  reduce = 0%\n",
      "2018-06-16 23:25:03,598 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 2.14 sec\n",
      "2018-06-16 23:25:10,868 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 4.32 sec\n",
      "MapReduce Total cumulative CPU time: 4 seconds 320 msec\n",
      "Ended Job = job_1529173785517_0056\n",
      "Launching Job 2 out of 3\n",
      "Number of reduce tasks not specified. Estimated from input data size: 1\n",
      "In order to change the average load for a reducer (in bytes):\n",
      "  set hive.exec.reducers.bytes.per.reducer=<number>\n",
      "In order to limit the maximum number of reducers:\n",
      "  set hive.exec.reducers.max=<number>\n",
      "In order to set a constant number of reducers:\n",
      "  set mapreduce.job.reduces=<number>\n",
      "Starting Job = job_1529173785517_0057, Tracking URL = http://c8bd72c66014:8088/proxy/application_1529173785517_0057/\n",
      "Kill Command = /opt/hadoop/bin/hadoop job  -kill job_1529173785517_0057\n",
      "Hadoop job information for Stage-2: number of mappers: 1; number of reducers: 1\n",
      "2018-06-16 23:25:26,210 Stage-2 map = 0%,  reduce = 0%\n",
      "2018-06-16 23:25:32,470 Stage-2 map = 100%,  reduce = 0%, Cumulative CPU 1.89 sec\n",
      "2018-06-16 23:25:39,750 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 5.15 sec\n",
      "MapReduce Total cumulative CPU time: 5 seconds 150 msec\n",
      "Ended Job = job_1529173785517_0057\n",
      "Launching Job 3 out of 3\n",
      "Number of reduce tasks determined at compile time: 1\n",
      "In order to change the average load for a reducer (in bytes):\n",
      "  set hive.exec.reducers.bytes.per.reducer=<number>\n",
      "In order to limit the maximum number of reducers:\n",
      "  set hive.exec.reducers.max=<number>\n",
      "In order to set a constant number of reducers:\n",
      "  set mapreduce.job.reduces=<number>\n",
      "Starting Job = job_1529173785517_0058, Tracking URL = http://c8bd72c66014:8088/proxy/application_1529173785517_0058/\n",
      "Kill Command = /opt/hadoop/bin/hadoop job  -kill job_1529173785517_0058\n",
      "Hadoop job information for Stage-3: number of mappers: 1; number of reducers: 1\n",
      "2018-06-16 23:25:54,801 Stage-3 map = 0%,  reduce = 0%\n",
      "2018-06-16 23:26:01,129 Stage-3 map = 100%,  reduce = 0%, Cumulative CPU 1.57 sec\n",
      "2018-06-16 23:26:09,403 Stage-3 map = 100%,  reduce = 100%, Cumulative CPU 4.43 sec\n",
      "MapReduce Total cumulative CPU time: 4 seconds 430 msec\n",
      "Ended Job = job_1529173785517_0058\n",
      "MapReduce Jobs Launched: \n",
      "Stage-Stage-1: Map: 1  Reduce: 1   Cumulative CPU: 4.32 sec   HDFS Read: 10855 HDFS Write: 206 SUCCESS\n",
      "Stage-Stage-2: Map: 1  Reduce: 1   Cumulative CPU: 5.15 sec   HDFS Read: 5974 HDFS Write: 211 SUCCESS\n",
      "Stage-Stage-3: Map: 1  Reduce: 1   Cumulative CPU: 4.43 sec   HDFS Read: 5730 HDFS Write: 16 SUCCESS\n",
      "Total MapReduce CPU Time Spent: 13 seconds 900 msec\n",
      "OK\n",
      "2008\t2008-10\t73\n",
      "Time taken: 86.374 seconds, Fetched: 1 row(s)\n"
     ]
    }
   ],
   "source": [
    "#! head /data/stackexchange1000/posts/part-00000\n",
    "! hive  -S -f task1a.hql\n",
    "! hive  -S -f task1b.hql\n",
    "! hive  -f task1c.hql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
